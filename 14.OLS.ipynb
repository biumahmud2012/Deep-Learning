{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models and OLS\n",
    "\n",
    "**Regression** refers to the prediction of a continuous variable (income, age, height, etc.) using a dataset's features. A **linear model** is a model of the form:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_K x_K + \\epsilon$$\n",
    "\n",
    "Here $\\epsilon$ is an **error term**; the predicted value for $y$ is given by $\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_K x_K$, so $y - \\hat{y} = \\epsilon$.\n",
    "\n",
    "$\\epsilon$ is almost never zero, so for regression we must measure \"accuracy\" differently. The **sum of squared errors (SSE)** is the sum $\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2$ (letting $y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + ... + \\beta_K x_{K,i} + \\epsilon_i$ and $\\hat{y}_i$ defined analogously). We might define the \"most accurate\" regression model as the model that minimizes the SSE. However, when measuring performance, the **mean squared error (MSE)** is often used. The MSE is given by $\\frac{\\text{SSE}}{n} = \\frac{1}{n}\\sum_{i = 1}^{n} (y_i - \\hat{y}_i)^2$.\n",
    "\n",
    "**Ordinary least squares (OLS)** is a procedure for finding a linear model that minimizes the SSE on a dataset. This is the simplest procedure for fitting a linear model on a dataset. To evaluate the model's performance we may split a dataset into training and test set, and evaluate the trained model's performance by computing the MSE of the model's predictions on the test set. If the model has a high MSE on both the training and test set, it's underfitting. If it has a small MSE on the training set and a high MSE on the test set, it is overfitting.\n",
    "\n",
    "With OLS the most important decision is which features to use in prediction and how to use them. \"Linear\" means linear in coefficients only; these models can handle many kinds of functions. (The models $\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^2$ and $\\hat{y} = \\beta_0 + \\beta_1 \\log(x)$ are linear, but $\\hat{y} = \\frac{\\beta_0}{1 + \\beta_1 x}$ is not.) Many approaches exist for deciding which features to include. For now we will only use cross-validation.\n",
    "\n",
    "## Fitting a Linear Model with OLS\n",
    "\n",
    "OLS is supported by the `LinearRegression` object in **scikit-learn**, while the function `mean_squared_error()` computes the MSE.\n",
    "\n",
    "I will be using OLS to find a linear model for predicting home prices in the Boston house price dataset, created below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.32000000e-03,   1.80000000e+01,   2.31000000e+00,\n",
       "          0.00000000e+00,   5.38000000e-01,   6.57500000e+00,\n",
       "          6.52000000e+01,   4.09000000e+00,   1.00000000e+00,\n",
       "          2.96000000e+02,   1.53000000e+01,   3.96900000e+02,\n",
       "          4.98000000e+00],\n",
       "       [  2.73100000e-02,   0.00000000e+00,   7.07000000e+00,\n",
       "          0.00000000e+00,   4.69000000e-01,   6.42100000e+00,\n",
       "          7.89000000e+01,   4.96710000e+00,   2.00000000e+00,\n",
       "          2.42000000e+02,   1.78000000e+01,   3.96900000e+02,\n",
       "          9.14000000e+00],\n",
       "       [  2.72900000e-02,   0.00000000e+00,   7.07000000e+00,\n",
       "          0.00000000e+00,   4.69000000e-01,   7.18500000e+00,\n",
       "          6.11000000e+01,   4.96710000e+00,   2.00000000e+00,\n",
       "          2.42000000e+02,   1.78000000e+01,   3.92830000e+02,\n",
       "          4.03000000e+00],\n",
       "       [  3.23700000e-02,   0.00000000e+00,   2.18000000e+00,\n",
       "          0.00000000e+00,   4.58000000e-01,   6.99800000e+00,\n",
       "          4.58000000e+01,   6.06220000e+00,   3.00000000e+00,\n",
       "          2.22000000e+02,   1.87000000e+01,   3.94630000e+02,\n",
       "          2.94000000e+00],\n",
       "       [  6.90500000e-02,   0.00000000e+00,   2.18000000e+00,\n",
       "          0.00000000e+00,   4.58000000e-01,   7.14700000e+00,\n",
       "          5.42000000e+01,   6.06220000e+00,   3.00000000e+00,\n",
       "          2.22000000e+02,   1.87000000e+01,   3.96900000e+02,\n",
       "          5.33000000e+00]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_obj = load_boston()\n",
    "data, price = boston_obj.data, boston_obj.target\n",
    "data[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 24. ,  21.6,  34.7,  33.4,  36.2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.74460000e-01,   0.00000000e+00,   1.05900000e+01,\n",
       "          1.00000000e+00,   4.89000000e-01,   5.96000000e+00,\n",
       "          9.21000000e+01,   3.87710000e+00,   4.00000000e+00,\n",
       "          2.77000000e+02,   1.86000000e+01,   3.93250000e+02,\n",
       "          1.72700000e+01],\n",
       "       [  5.82115000e+00,   0.00000000e+00,   1.81000000e+01,\n",
       "          0.00000000e+00,   7.13000000e-01,   6.51300000e+00,\n",
       "          8.99000000e+01,   2.80160000e+00,   2.40000000e+01,\n",
       "          6.66000000e+02,   2.02000000e+01,   3.93820000e+02,\n",
       "          1.02900000e+01],\n",
       "       [  6.32000000e-03,   1.80000000e+01,   2.31000000e+00,\n",
       "          0.00000000e+00,   5.38000000e-01,   6.57500000e+00,\n",
       "          6.52000000e+01,   4.09000000e+00,   1.00000000e+00,\n",
       "          2.96000000e+02,   1.53000000e+01,   3.96900000e+02,\n",
       "          4.98000000e+00],\n",
       "       [  7.25800000e-01,   0.00000000e+00,   8.14000000e+00,\n",
       "          0.00000000e+00,   5.38000000e-01,   5.72700000e+00,\n",
       "          6.95000000e+01,   3.79650000e+00,   4.00000000e+00,\n",
       "          3.07000000e+02,   2.10000000e+01,   3.90950000e+02,\n",
       "          1.12800000e+01],\n",
       "       [  4.29400000e-02,   2.80000000e+01,   1.50400000e+01,\n",
       "          0.00000000e+00,   4.64000000e-01,   6.24900000e+00,\n",
       "          7.73000000e+01,   3.61500000e+00,   4.00000000e+00,\n",
       "          2.70000000e+02,   1.82000000e+01,   3.96900000e+02,\n",
       "          1.05900000e+01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train, data_test, price_train, price_test = train_test_split(data, price)\n",
    "data_train[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 21.7,  20.2,  24. ,  18.2,  20.6])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go ahead and use all features for prediction in our first linear model. (In general this does *not* necessarily produce better models; some features may introduce only noise that makes prediction *more* difficult, not less.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 38.4845554])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols1 = LinearRegression()\n",
    "ols1.fit(data_train, price_train)    # Fitting a linear model\n",
    "ols1.predict([[    # An example prediction\n",
    "    1,      # Per capita crime rate\n",
    "    25,     # Proportion of land zoned for large homes\n",
    "    5,      # Proportion of land zoned for non-retail business\n",
    "    1,      # Tract bounds the Charles River\n",
    "    0.3,    # NOX concentration\n",
    "    10,     # Average number of rooms per dwelling\n",
    "    2,      # Proportion of owner-occupied units built prior to 1940\n",
    "    10,     # Weighted distance to employment centers\n",
    "    3,      # Index for highway accessibility\n",
    "    400,    # Tax rate\n",
    "    15,     # Pupil/teacher ratio\n",
    "    200,    # Index for number of blacks\n",
    "    5       # % lower status of population\n",
    "]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 22.93387135,  23.01140472,  29.89639538,  18.66387952,  27.20777345])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predprice = ols1.predict(data_train)\n",
    "predprice[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.682766677837442"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(price_train, predprice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.7626428249279247"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(price_train, predprice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The square root of the mean squared error can be interpreted as the average amount of error; in this case, the average difference between homes' actual and predicted prices. (This is almost the standard deviation of the error.)\n",
    "\n",
    "For cross-validation, I will use `cross_val_score()`, which performs the entire cross-validation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-25.52170955017451"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ols2 = LinearRegression()\n",
    "ols_cv_mse = cross_val_score(ols2, data_train, price_train, scoring='neg_mean_squared_error', cv=10)\n",
    "ols_cv_mse.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above number is the negative average MSE for cross-validation (minimizing MSE is equivalent to maximizing the negative MSE). This is close to our in-sample MSE.\n",
    "\n",
    "Let's now see the MSE for the fitted model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.972466191523765"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testpredprice = ols1.predict(data_test)\n",
    "mean_squared_error(price_test, testpredprice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5795705247898262"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(price_test, testpredprice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting is minimal, it seems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
